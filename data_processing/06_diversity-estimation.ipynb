{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating repertoire diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import ast\n",
    "from datetime import datetime\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/subjects.txt') as f:\n",
    "    subjects = sorted(f.read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diversity estimation functions\n",
    "\n",
    "In order to perform the diversity estimate using Recon, you must first download and install [Recon](https://arnaoutlab.github.io/Recon/). If necessary, update the `recon_path` variable below, which should point to the `recon.py` file.  \n",
    "\n",
    "Additionally, because Recon requires Python 2.7 and your default Python executable (if you're able to run this notebook) is likely Python 3.x, we need directions to a Python 2.7 excecutable. If you only have Python 3.x and would prefer to keep your default Python executable unchanged, one way to easily install Python 2.7 on a system with a pre-existing Python 3.x install is to use [Anaconda](https://www.anaconda.com/download/). When installing Python 2.7 via Anaconda, respond `no` when prompted to \"prepend the Anaconda2 install location to PATH\". This will ensure that your default Python executable will remain unchanged. Make sure to update the `python_path` variable below to point to a Python 2.7 executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_path = '~/recon.py'\n",
    "python_path = '~/anaconda2/bin/python'\n",
    "recon_raw_data_path = './data/user-calculated_raw-recon-data'\n",
    "diversity_output_dir = './data/user-calculated_diversity-estimation'\n",
    "\n",
    "recon_path = os.path.expanduser(recon_path)\n",
    "python_path = os.path.expanduser(python_path)\n",
    "make_dir(recon_raw_data_path)\n",
    "make_dir(diversity_output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_chao2(counts, m=6):\n",
    "    m = float(m)\n",
    "    s_obs = float(sum(counts.values()))\n",
    "    if all([1 in counts, 2 in counts]):\n",
    "        q1 = float(counts[1])\n",
    "        q2 = float(counts[2])\n",
    "    else:\n",
    "        q1 = float(counts['1'])\n",
    "        q2 = float(counts['2'])\n",
    "    return s_obs + ((m - 1) / m) * ((q1 * (q1 - 1)) / (2 * (q2 + 1)))\n",
    "\n",
    "\n",
    "def do_recon(counts=None, infile=None, outfile='/dev/null', with_ci=False):\n",
    "    if counts is not None and infile is None:\n",
    "        inhandle = tempfile.NamedTemporaryFile(dir='/tmp', delete=False, mode='w')\n",
    "        infile = inhandle.name\n",
    "        _dlist = ['{}\\t{}'.format(k, v) for k, v in sorted(counts.items(), key=lambda x: int(x[0]))]\n",
    "        inhandle.write('\\n'.join(_dlist))\n",
    "        inhandle.close()\n",
    "    recon_cmd = '{} {} -R -c -o {} {}'.format(python_path, recon_path, outfile, infile)\n",
    "    p = sp.Popen(recon_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    data = ast.literal_eval(stdout.decode().strip().split('\\n')[1])\n",
    "    count = data[2] + sum(data[3].values())\n",
    "    return count\n",
    "\n",
    "\n",
    "def confidence_interval(data, interval=0.95):\n",
    "    mu = np.mean(data)\n",
    "    sigma = np.std(data)\n",
    "    i = stats.norm.interval(interval, loc=mu, scale=sigma)\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clonotype diversity estimation (by subject)\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data location\n",
    "data_path = './data/equal_fraction_downsampling/'\n",
    "clonotype_data_filename = 'clonotype-downsampling_duplicate-counts_vj-aa.txt'\n",
    "clonotype_data_file = os.path.join(data_path, clonotype_data_filename)\n",
    "\n",
    "# read data file\n",
    "clonotype_data = {}\n",
    "with open(clonotype_data_file) as f:\n",
    "    subject_samples = f.read().split('#')[1:]\n",
    "    for ss in subject_samples:\n",
    "        subject = ss.split('\\n')[0]\n",
    "        clonotype_data[subject] = {}\n",
    "        subsamples = '\\n'.join(ss.split('\\n'))[1:].split('>')\n",
    "        for subsample in subsamples[1:]:\n",
    "            subsample_size = float(subsample.split('\\n')[0])\n",
    "            clonotype_data[subject][subsample_size] = []\n",
    "            vals = subsample.split('\\n')[1:]\n",
    "            for val in vals:\n",
    "                if not val.split():\n",
    "                    continue\n",
    "                d = [v.split(':') for v in val.split()]\n",
    "                clonotype_data[subject][subsample_size].append({int(k): int(v) for k, v in d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate diversity\n",
    "\n",
    "While diversity estimation with the Chao2 estimator is quite fast (only a second or two to process all downsamples from all subjects), Recon is much more compute intensive. Even with multiprocessing, the code block below requires about half an hour to run on a modern Macbook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "  CHAO\n",
      "========\n",
      "\n",
      "316188\n",
      "326650\n",
      "326651\n",
      "326713\n",
      "326737\n",
      "326780\n",
      "326797\n",
      "326907\n",
      "327059\n",
      "D103\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chao diversity\n",
    "print('========')\n",
    "print('  CHAO')\n",
    "print('========')\n",
    "print('')\n",
    "chao_diversity = {}\n",
    "for subject in clonotype_data:\n",
    "    print(subject)\n",
    "    chao_diversity[subject] = {}\n",
    "    for subsample in clonotype_data[subject]:\n",
    "        chao_diversity[subject][subsample] = []\n",
    "        for iteration in clonotype_data[subject][subsample]:\n",
    "            div = do_chao2(iteration)\n",
    "            chao_diversity[subject][subsample].append(div)\n",
    "chao_file = os.path.join(diversity_output_dir, 'single-subject_clonotypes_chao2.json')\n",
    "with open(chao_file, 'w') as f:\n",
    "    json.dump(chao_diversity, f)\n",
    "print('\\n\\n')\n",
    "\n",
    "# recon diversity\n",
    "print('=========')\n",
    "print('  RECON')\n",
    "print('=========')\n",
    "print('')\n",
    "recon_diversity = {}\n",
    "p = mp.Pool(maxtasksperchild=1)\n",
    "for subject in data:\n",
    "    print(subject)\n",
    "    start_time = datetime.now()\n",
    "    subsample_count = len(clonotype_data[subject].keys())\n",
    "    progress_bar(0, subsample_count, start_time=start_time)\n",
    "    recon_diversity[subject] = {}\n",
    "    for scount, subsample in enumerate(clonotype_data[subject].keys()):\n",
    "        async_results = []\n",
    "        for i, iteration in enumerate(clonotype_data[subject][subsample]):\n",
    "            outfile = os.path.join(recon_raw_data_path, '{}_{}_{}'.format(subject, subsample, i))\n",
    "            async_results.append(p.apply_async(do_recon, args=(iteration, None, outfile)))\n",
    "        recon_diversity[subject][subsample] = [ar.get() for ar in async_results]\n",
    "        progress_bar(scount + 1, subsample_count, start_time=start_time)\n",
    "    print('')\n",
    "recon_file = os.path.join(diversity_output_dir, 'single-subject_clonotypes_recon.json')\n",
    "with open(recon_file, 'w') as f:\n",
    "    json.dump(recon_diversity, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clonotype diversity estimation (multi-subject pools)\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "cross_subject_data_dir = './data/cross-subject_clonotype_duplicate-counts/'\n",
    "files = [f for f in list_files(cross_subject_data_dir) if 'occurrence-counts.txt' in f]\n",
    "\n",
    "# organize by group size\n",
    "files_by_subject_count = {i: [] for i in range(1, 11)}\n",
    "for f in files:\n",
    "    num = len(os.path.basename(f).split('_')[0].split('-'))\n",
    "    files_by_subject_count[num].append(f)\n",
    "    \n",
    "# load data\n",
    "clonotype_counts = {}\n",
    "for num_subs in files_by_subject_count.keys():\n",
    "    clonotype_counts[num_subs] = []\n",
    "    for ifile in files_by_subject_count[num_subs]:\n",
    "        _counts = {}\n",
    "        with open(ifile) as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                n, c = line.strip().split()\n",
    "                _counts[n] = int(c)\n",
    "        clonotype_counts[num_subs].append(_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_subject_diversity(counts, estimator=None):\n",
    "    means = []\n",
    "    lognorm_means = []\n",
    "    lowers = []\n",
    "    uppers = []\n",
    "    lognorm_lowers = []\n",
    "    lognorm_uppers = []\n",
    "    raw_xs = []\n",
    "    raw_ys = []\n",
    "    \n",
    "    p = mp.Pool(maxtasksperchild=1)\n",
    "\n",
    "    for num_subjects in sorted(list(counts.keys())):\n",
    "        print('Group size:', num_subjects)\n",
    "        divs = []\n",
    "        async_results = []\n",
    "        progress_bar(0, len(counts[num_subjects]))\n",
    "        for _counts in counts[num_subjects]:\n",
    "            if estimator.lower() == 'chao':\n",
    "                func = do_chao2\n",
    "                m = int(num_subjects) if int(num_subjects) > 1 else 6\n",
    "                args = (_counts, int(m))\n",
    "            else:\n",
    "                func = do_recon\n",
    "                args = (_counts, )\n",
    "            async_results.append(p.apply_async(func, args=args))\n",
    "        monitor_mp_jobs(async_results)\n",
    "        divs = [ar.get() for ar in async_results]\n",
    "        \n",
    "        raw_xs.extend([num_subjects] * len(divs))\n",
    "        raw_ys.extend(divs)\n",
    "        if len(divs) == 1:\n",
    "            mean = divs[0]\n",
    "            lognorm_mean = np.log10(mean)\n",
    "            lower = divs[0]\n",
    "            upper = divs[0]\n",
    "            lognorm_lower, lognorm_upper = np.log10(upper), np.log10(lower)\n",
    "        else:\n",
    "            mean = np.mean(divs)\n",
    "            lognorm_mean = np.log10(mean)\n",
    "            lower, upper = confidence_interval(divs)\n",
    "            lognorm_lower, lognorm_upper = confidence_interval([np.log10(d) for d in divs])\n",
    "        means.append(mean)\n",
    "        lognorm_means.append(lognorm_mean)\n",
    "        lowers.append(lower)\n",
    "        uppers.append(upper)\n",
    "        lognorm_lowers.append(lognorm_lower)\n",
    "        lognorm_uppers.append(lognorm_upper)\n",
    "        print('\\n')\n",
    "    ret = {'raw_xs': raw_xs, 'raw_ys': raw_ys,\n",
    "           'means': means, 'uppers': uppers, 'lowers': lowers,\n",
    "           'lognorm_means': lognorm_means, 'lognorm_uppers': lognorm_uppers, 'lognorm_lowers': lognorm_lowers}\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 1\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 2\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 3\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 4\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 5\n",
      "(252/252) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 6\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 7\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 8\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 9\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 10\n",
      "(1/1) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chao_diversity = cross_subject_diversity(clonotype_counts, estimator='chao')\n",
    "\n",
    "with open('./data/diversity_estimation/cross-subject_clonotypes_chao2.json', 'w') as f:\n",
    "    json.dump(chao_diversity, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 1\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 2\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 3\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 4\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 5\n",
      "(252/252) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 6\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 7\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 8\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 9\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 10\n",
      "(1/1) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recon_diversity = cross_subject_diversity(clonotype_counts, estimator='recon')\n",
    "\n",
    "with open('./data/diversity_estimation/cross-subject_clonotypes_recon.json', 'w') as f:\n",
    "    json.dump(recon_diversity, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence diversity estimation (by subject)\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data location\n",
    "data_path = './data/equal_fraction_downsampling/'\n",
    "sequence_data_filename = 'sequence-downsampling_duplicate-counts_nt-seq.txt'\n",
    "sequence_data_file = os.path.join(data_path, sequence_data_filename)\n",
    "\n",
    "# read data file\n",
    "sequence_data = {}\n",
    "with open(sequence_data_file) as f:\n",
    "    subject_samples = f.read().split('#')[1:]\n",
    "    for ss in subject_samples:\n",
    "        subject = ss.split('\\n')[0]\n",
    "        sequence_data[subject] = {}\n",
    "        subsamples = '\\n'.join(ss.split('\\n'))[1:].split('>')\n",
    "        for subsample in subsamples[1:]:\n",
    "            subsample_size = float(subsample.split('\\n')[0])\n",
    "            sequence_data[subject][subsample_size] = []\n",
    "            vals = subsample.split('\\n')[1:]\n",
    "            for val in vals:\n",
    "                if not val.split():\n",
    "                    continue\n",
    "                d = [v.split(':') for v in val.split()]\n",
    "                sequence_data[subject][subsample_size].append({int(k): int(v) for k, v in d})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate diversity\n",
    "\n",
    "While diversity estimation with the Chao2 estimator is quite fast (only a second or two to process all downsamples from all subjects), Recon is much more compute intensive. Even with multiprocessing, the code block below requires just under an hour to run on a modern Macbook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========\n",
      "  CHAO\n",
      "========\n",
      "\n",
      "316188\n",
      "326650\n",
      "326651\n",
      "326713\n",
      "326737\n",
      "326780\n",
      "326797\n",
      "326907\n",
      "327059\n",
      "D103\n",
      "\n",
      "\n",
      "\n",
      "=========\n",
      "  RECON\n",
      "=========\n",
      "\n",
      "316188\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:56)  \n",
      "326650\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:37)  \n",
      "326651\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (04:14)  \n",
      "326713\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (04:08)  \n",
      "326737\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:19)  \n",
      "326780\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (04:02)  \n",
      "326797\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:54)  \n",
      "326907\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:35)  \n",
      "327059\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:55)  \n",
      "D103\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  (03:05)  \n"
     ]
    }
   ],
   "source": [
    "# chao diversity\n",
    "print('========')\n",
    "print('  CHAO')\n",
    "print('========')\n",
    "print('')\n",
    "chao_diversity = {}\n",
    "for subject in sequence_data:\n",
    "    print(subject)\n",
    "    chao_diversity[subject] = {}\n",
    "    for subsample in sequence_data[subject]:\n",
    "        chao_diversity[subject][subsample] = []\n",
    "        for iteration in sequence_data[subject][subsample]:\n",
    "            div = do_chao2(iteration)\n",
    "            chao_diversity[subject][subsample].append(div)\n",
    "chao_file = os.path.join(diversity_output_dir, 'single-subject_sequences_chao2.json')\n",
    "with open(chao_file, 'w') as f:\n",
    "    json.dump(chao_diversity, f)\n",
    "print('\\n\\n')\n",
    "\n",
    "\n",
    "# recon diversity\n",
    "print('=========')\n",
    "print('  RECON')\n",
    "print('=========')\n",
    "print('')\n",
    "recon_diversity = {}\n",
    "p = mp.Pool(maxtasksperchild=1)\n",
    "for subject in data:\n",
    "    print(subject)\n",
    "    start_time = datetime.now()\n",
    "    subsample_count = len(sequence_data[subject].keys())\n",
    "    progress_bar(0, subsample_count, start_time=start_time)\n",
    "    recon_diversity[subject] = {}\n",
    "    for scount, subsample in enumerate(sequence_data[subject].keys()):\n",
    "        async_results = []\n",
    "        for i, iteration in enumerate(sequence_data[subject][subsample]):\n",
    "            outfile = os.path.join(recon_raw_data_path, '{}_{}_{}'.format(subject, subsample, i))\n",
    "            async_results.append(p.apply_async(do_recon, args=(iteration, None, outfile)))\n",
    "        recon_diversity[subject][subsample] = [ar.get() for ar in async_results]\n",
    "        progress_bar(scount + 1, subsample_count, start_time=start_time)\n",
    "    print('')\n",
    "recon_file = os.path.join(diversity_output_dir, 'single-subject_sequences_recon.json')\n",
    "with open(recon_file, 'w') as f:\n",
    "    json.dump(recon_diversity, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence diversity estimation (cross-subject pools)\n",
    "\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input files\n",
    "cross_subject_data_dir = './data/cross-subject_sequence_duplicate-counts/'\n",
    "files = [f for f in list_files(cross_subject_data_dir) if 'occurrence-counts.txt' in f]\n",
    "\n",
    "# organize by group size\n",
    "files_by_subject_count = {i: [] for i in range(1, 11)}\n",
    "for f in files:\n",
    "    num = len(os.path.basename(f).split('_')[0].split('-'))\n",
    "    files_by_subject_count[num].append(f)\n",
    "    \n",
    "# load data\n",
    "sequence_counts = {}\n",
    "for num_subs in files_by_subject_count.keys():\n",
    "    sequence_counts[num_subs] = []\n",
    "    for ifile in files_by_subject_count[num_subs]:\n",
    "        _counts = {}\n",
    "        with open(ifile) as f:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                n, c = line.strip().split()\n",
    "                _counts[n] = int(c)\n",
    "        sequence_counts[num_subs].append(_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 1\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 2\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 3\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 4\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 5\n",
      "(252/252) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 6\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 7\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 8\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 9\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 10\n",
      "(1/1) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chao_diversity = cross_subject_diversity(sequence_counts, estimator='chao')\n",
    "\n",
    "with open('./data/diversity_estimation/cross-subject_sequences_chao2.json', 'w') as f:\n",
    "    json.dump(chao_diversity, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group size: 1\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 2\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 3\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 4\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 5\n",
      "(252/252) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 6\n",
      "(210/210) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 7\n",
      "(120/120) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 8\n",
      "(45/45) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 9\n",
      "(10/10) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n",
      "Group size: 10\n",
      "(1/1) ||||||||||||||||||||||||||||||||||||||||||||||||||||  100%  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recon_diversity = cross_subject_diversity(sequence_counts, estimator='recon')\n",
    "\n",
    "with open('./data/diversity_estimation/cross-subject_sequences_recon.json', 'w') as f:\n",
    "    json.dump(recon_diversity, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
