{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morisita-Horn similarity calculation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-defined options\n",
    "\n",
    "By default, the size of the bootstrap samples will increase exponentially, as the similarity plots will be drawn with a logarithmic x-axis. However, the option is also given to draw bootstrap samples that increase linearly in size rather than exponentially. The following options may be adjusted depending on the desired output:\n",
    "\n",
    "  * `iterations` is the number of replicate samplings for each subsample size. Default is `10`.  \n",
    "  \n",
    "  * `max_power_of_10` is the highest exponent of 10 for which subsamples will be drawn. For example, the default value of `7` means that the largest bootstrap sample will be `10^7`, or 10 million, sequences. The lowest acceptable value is `2`, as subsampling fewer than 100 sequences is not especially useful.  \n",
    "  \n",
    "  * `subsample_fraction` is the fraction of each `power_of_10` multiple at which the bootstrap sample size increases. For example, the default value of `0.3` results in the following multipliers: `[1.0, 1.3, 1.6, 1.9]`. For a `power_of_10` of 10^6, for example, a `subsample_fraction` of `0.3` would result in the following bootstrap sample sizes: `1.0x10^6, 1.3x10^6, 1.6x10^6, and 1,9x10^6`.  \n",
    "  \n",
    "  * `subsample_size` is the size multiple for each subsample. By default (if `subsample_fraction` is provided), this will not be used. This option is only provided in case you would prefer the subsample size pools to increase in linear fashion, rather than exponentially.\n",
    "\n",
    "Note that the data directory (`'./data/techrep-merged_vj-cdr3len_no-header/'`) is not present in this Github repo, as the size of the files far exceeds what is allowed by Github. You can download a compressed archive containing the appropriate data files [**here**](http://burtonlab.s3.amazonaws.com/GRP_github_data/techrep-merged_vj-cdr3len_no-header.tar.gz). Decompressing the archive inside of `./data` (the \"data\" directory found in the same parent directory as this notebook) should allow you to run the following code without alteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 10\n",
    "max_power_of_10 = 7\n",
    "subsample_fraction = 0.3\n",
    "subsample_size = 25000\n",
    "\n",
    "data_dir = './data/techrep-merged_vj-cdr3len_no-header/'\n",
    "temp_dir = './data/temp/'\n",
    "output_dir = './data/user-calculated_mh_similarity/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects and directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_files_dir = os.path.join(output_dir, 'individual_comparisons')\n",
    "make_dir(temp_dir)\n",
    "make_dir(output_dir)\n",
    "make_dir(individual_files_dir)\n",
    "\n",
    "with open('../data_processing/data/subjects.txt') as f:\n",
    "    subjects = sorted(f.read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morisita-Horn similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mh_similarity(sample1, sample2):\n",
    "    '''\n",
    "    Calculates the Marista-Horn similarity for two samples.\n",
    "\n",
    "    .. note:\n",
    "\n",
    "        sample1 and sample2 should be the same length, and\n",
    "        the sum of each sample should be greater than 0.\n",
    "\n",
    "    Args:\n",
    "\n",
    "        sample1 (list): list of frequencies for sample 1\n",
    "        \n",
    "        sample2 (list): list of frequencies for sample 2\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        float: Marista-Horn similarity (between 0 and 1)\n",
    "    '''\n",
    "    X = sum(sample1)\n",
    "    Y = sum(sample2)\n",
    "    XY = X * Y\n",
    "    sumXiYi = 0\n",
    "    sumXiSq = 0\n",
    "    sumYiSq = 0\n",
    "    for x, y in zip(sample1, sample2):\n",
    "        sumXiYi += x * y\n",
    "        sumXiSq += x * x\n",
    "        sumYiSq += y * y\n",
    "    num = 2 * sumXiYi\n",
    "    denom = (float(sumXiSq) / (X * X) + float(sumYiSq) / (Y * Y)) * XY\n",
    "    return 1. * num / denom\n",
    "\n",
    "\n",
    "def load_data(files):\n",
    "    '''\n",
    "    Loads VJ-CDR3len data from a list of files corresponding to a single subject.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        files (list): a list of files containing data to load. Data will be\n",
    "                      loaded for all files and returned as a single list.\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list: a combined list of data from all of the files\n",
    "    '''\n",
    "    data = []\n",
    "    for f in files:\n",
    "        with open(f) as of:\n",
    "            for line in of:\n",
    "                _d = '_'.join(line.strip().split())\n",
    "                if _d:\n",
    "                    data.append(_d)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_subsample_sizes():\n",
    "    '''\n",
    "    Returns a list of subsample sizes, based on user-defined subsampling options.\n",
    "    '''\n",
    "    if subsample_fraction is not None:\n",
    "        sizes = []\n",
    "        for mpt in range(2, max_power_of_10 + 1):\n",
    "            start = 10**(mpt - 1)\n",
    "            end = 10**mpt\n",
    "            step = int(10**mpt * float(subsample_fraction))\n",
    "            sizes += list(range(start, end, step))\n",
    "        sizes.append(10**mpt)\n",
    "    else:\n",
    "        sizes = range(subsample_step, 10 ** max_power_of_10, subsample_step)\n",
    "    return sizes\n",
    "\n",
    "\n",
    "def compute_frequencies(data, iterations, size):\n",
    "    '''\n",
    "    Subsamples a dataset (with replacement) and computes the VJ-CDR3len\n",
    "    frequency for each bootstrap sample.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        data (list): a list of antibody sequences collapsed to just VJ-CDR3len\n",
    "        \n",
    "        iterations (int): the number of bootstrap samplings to be performed\n",
    "        \n",
    "        size (int): the size (in sequences) of each bootstrap sample\n",
    "    \n",
    "    Returns:\n",
    "    \n",
    "        list(Counter): a list of VJ-CDR3len frequencies (as Counter objects)\n",
    "    '''\n",
    "    subsamples = np.random.choice(data, size=(iterations, size), replace=True)\n",
    "    freqs = []\n",
    "    for subsample in subsamples:\n",
    "        freqs.append(Counter(subsample))\n",
    "    return freqs\n",
    "\n",
    "\n",
    "def compute_similarity_for_single_size(sub1_data, sub2_data, iterations, size):\n",
    "    '''\n",
    "    For a single bootstrap sampling size, computes Morisita-Horn similarity of\n",
    "    two datasets.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        sub1_data (list): a list of VJ-CDR3len values\n",
    "        \n",
    "        sub2_data (list): a list of VJ-CDR3len values\n",
    "        \n",
    "        iterations (int): the number of iterations to be performed\n",
    "        \n",
    "        size (int): size (in sequences) of each bootstrap sampling\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        int: the size of each bootstrap sampling\n",
    "        \n",
    "        list: a list of Morisita-Horn similarities, of length `iterations`\n",
    "    '''\n",
    "    similarities = []\n",
    "    sub1_freqs = get_frequencies(sub1_data, iterations, size)\n",
    "    sub2_freqs = get_frequencies(sub2_data, iterations, size)\n",
    "    for s1, s2 in zip(sub1_freqs, sub2_freqs):\n",
    "        freq_df = pd.DataFrame({'sub1': s1, 'sub2': s2}).fillna(0)\n",
    "        similarities.append(mh_similarity(freq_df['sub1'], freq_df['sub2']))\n",
    "    return size, similarities\n",
    "\n",
    "\n",
    "def calculate_similarities(subject1, subject2, iterations, sizes):\n",
    "    '''\n",
    "    Performs Morisita-Horn similarity calculations on VJ-CDR3len data for two subjects.\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        subject1 (str): name of subject 1\n",
    "        \n",
    "        subject2 (str): name of subject 2\n",
    "        \n",
    "        iterations (int): number of iterations to be performed for each bootstrap sample size\n",
    "        \n",
    "        sizes (list(int)): a list of bootstrap sample sizes\n",
    "        \n",
    "    Returns:\n",
    "    \n",
    "        sub_header (str): a header line containing subject information\n",
    "        \n",
    "        similarities (dict): similarity scores, with the dict keys being sample sizes\n",
    "                             and values being lists of similarity scores of length `iterations`\n",
    "    '''\n",
    "    sub_header = '#{} {}'.format(subject1, subject2)\n",
    "    sub1_dir = os.path.join(data_dir, subject1)\n",
    "    sub2_dir = os.path.join(data_dir, subject2)\n",
    "    sub1_files = list_files(sub1_dir)\n",
    "    sub2_files = list_files(sub2_dir)\n",
    "    similarities = {}\n",
    "    output_data = [sub_header, ]\n",
    "    output_file = os.path.join(individual_files_dir, '{}-{}'.format(subject1, subject2))\n",
    "    \n",
    "    # load all of the files into memory\n",
    "    sub1 = os.path.basename(os.path.dirname(sub1_files[0]))\n",
    "    sub1_data = load_data(sub1_files)\n",
    "    sub2 = os.path.basename(os.path.dirname(sub2_files[0]))\n",
    "    sub2_data = load_data(sub2_files)\n",
    "    \n",
    "    for size in sizes:\n",
    "        similarities[size] = []\n",
    "        sub1_freqs = compute_frequencies(sub1_data, iterations, size)\n",
    "        sub2_freqs = compute_frequencies(sub2_data, iterations, size)\n",
    "        for s1, s2 in zip(sub1_freqs, sub2_freqs):\n",
    "            freq_df = pd.DataFrame({'sub1': s1, 'sub2': s2}).fillna(0)\n",
    "            similarities[size].append(mh_similarity(freq_df['sub1'], freq_df['sub2']))\n",
    "        output_data.append(' '.join([str(v) for v in [size] + similarities[size]]))\n",
    "        with open(output_file, 'w') as f:\n",
    "            f.write('\\n'.join(output_data))\n",
    "    return sub_header, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Calculate similarity\n",
    "\n",
    "Morisita-Horn similarity will be calculated for each pairwise combination of subjects (including self-comparisons). The process is multiprocess (one pairwise comparison per process) and will use as many cores as necessary to perform all comparisons (there are a total of 55 comparisons from 10 subjects, so the max number of cores that will be used is 55)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/55) |                                                  |  0%  (11:02)  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-7:\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 168, in calculate_similarities\n",
      "    sub2_freqs = compute_frequencies(sub2_data, iterations, size)\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 92, in compute_frequencies\n",
      "    subsamples = np.random.choice(data, size=(iterations, size), replace=True)\n",
      "  File \"mtrand.pyx\", line 1112, in mtrand.RandomState.choice\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 167, in calculate_similarities\n",
      "    sub1_freqs = compute_frequencies(sub1_data, iterations, size)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 92, in compute_frequencies\n",
      "    subsamples = np.random.choice(data, size=(iterations, size), replace=True)\n",
      "  File \"/anaconda3/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 168, in calculate_similarities\n",
      "    sub2_freqs = compute_frequencies(sub2_data, iterations, size)\n",
      "  File \"mtrand.pyx\", line 1112, in mtrand.RandomState.choice\n",
      "  File \"<ipython-input-4-1994935e457c>\", line 92, in compute_frequencies\n",
      "    subsamples = np.random.choice(data, size=(iterations, size), replace=True)\n",
      "  File \"mtrand.pyx\", line 1112, in mtrand.RandomState.choice\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d3e9be58dea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msubject1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0masync_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_similarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmonitor_mp_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masync_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/abutils/utils/jobs.py\u001b[0m in \u001b[0;36mmonitor_mp_jobs\u001b[0;34m(results, start_time, completion_string)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mjobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mfinished\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mfinished\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get a list of subsample sizes, based on user-defined options\n",
    "sizes = get_subsample_sizes()\n",
    "\n",
    "# get a list of all pairwise combinations of subjects (including self-comparison)\n",
    "combinations = list(itertools.combinations_with_replacement(subjects, 2))\n",
    "\n",
    "p = mp.Pool(processes=7, maxtasksperchild=1)\n",
    "start = datetime.now()\n",
    "async_results = []\n",
    "\n",
    "# initialize the progress bar\n",
    "jobs = len(combinations)\n",
    "progress_bar(0, jobs, start_time=start)\n",
    "\n",
    "# calculate the similarity score for each pairwise combination of subjects\n",
    "for subject1, subject2 in combinations:\n",
    "    async_results.append(p.apply_async(calculate_similarities, args=(subject1, subject2, iterations, sizes)))\n",
    "monitor_mp_jobs(async_results, start_time=start)\n",
    "results = [ar.get() for ar in async_results]\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine similarity files\n",
    "\n",
    "Each pairwise comparison resulted in a separate output file. Here we combine them into a single similarities file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_output_file = os.path.join(output_dir, 'mh-similarities_combined.txt')\n",
    "individual_files = list_files(individual_files_dir)\n",
    "\n",
    "with open(combined_output_file, 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "cat_cmd = 'for f in {}/*; do (cat \"${{f}}\"; echo) >> {}; done'.format(individual_files_dir.rstrip('/'), combined_output_file)\n",
    "p = sp.Popen(cat_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "stdout, stderr = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316188\n",
      "326650\n",
      "326651\n",
      "326713\n",
      "326737\n",
      "326780\n",
      "326797\n",
      "326907\n",
      "327059\n",
      "D103\n"
     ]
    }
   ],
   "source": [
    "for s in subjects:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
