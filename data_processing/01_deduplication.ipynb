{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonotype and sequence deduplication\n",
    "\n",
    "Starting with annotated sequence data (in AbStar's `minimal` output format), reduces sequences to clonotypes and collapses dupicate clonotypes.\n",
    "\n",
    "The [`abutils`](https://www.github.com/briney/abutils) Python package is required, and can be installed by running `pip install abutils`\n",
    "\n",
    "*NOTE: this notebook requires the use of the Unix command line tool `sort`. Thus, it requires a Unix-based operating system to run correctly (MacOS and most flavors of Linux should be fine). Running this notebook on Windows 10 may be possible using the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about) but we have not tested this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects, directories and data fields\n",
    "\n",
    "The input data (annotated sequences in [abstar's](https://github.com/briney/abstar) `minimal` format) is too large to be stored in a Github repository. A compressed archive of the data can be downloaded [**here**](http://burtonlab.s3.amazonaws.com/GRP_github_data/techrep-merged_minimal_no-header.tar.gz). The data file is fairly large (about 400GB uncompressed), so make sure you have enough space before downloading. Decompressing the archive from within the `data` directory (located in the same parent directory as this notebook) will allow the code in this notebook to run without modification. If you would prefer to store the input data somewhere else, be sure to modify the `raw_input_dir` path below.\n",
    "\n",
    "The data fields defined below correspond to the prosition in abstar's `minimal` format. If for some reason you have a differently formatted annotation file, change the field positions to suit your annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects\n",
    "with open('./data/subjects.txt') as f:\n",
    "    subjects = sorted(f.read().split())\n",
    "\n",
    "# directories\n",
    "raw_input_dir = './data/techrep-merged_minimal_no-header/'\n",
    "raw_clonotype_dir = './data/techrep-merged_vj-aa/'\n",
    "dedup_clonotype_dir = './data/dedup_techrep-merged_vj-aa/'\n",
    "dedup_sequence_dir = './data/dedup_techrep-merged_nt-seq/'\n",
    "logfile = './data/dedup.log'\n",
    "\n",
    "# data fields\n",
    "prod_field = 3\n",
    "v_field = 5\n",
    "j_field = 9\n",
    "cdr3aa_field = 12\n",
    "vdjnt_field = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (biological replicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_bioreps(files, raw_clonotype_dir, unique_clonotype_dir,\n",
    "                  raw_sequence_dir, unique_sequence_dir, log_file=None):\n",
    "    # set up output directories\n",
    "    make_dir(raw_clonotype_dir)\n",
    "    make_dir(unique_clonotype_dir)\n",
    "    make_dir(raw_sequence_dir)\n",
    "    make_dir(unique_sequence_dir)\n",
    "    \n",
    "    # process minimal output files\n",
    "    for _f in files:\n",
    "        print(os.path.basename(_f))\n",
    "        clonotype_output_data = []\n",
    "        sequence_output_data = []\n",
    "        raw_clonotype_file = os.path.join(raw_clonotype_dir, os.path.basename(_f))\n",
    "        unique_clonotype_file = os.path.join(unique_clonotype_dir, os.path.basename(_f))\n",
    "        raw_sequence_file = os.path.join(raw_sequence_dir, os.path.basename(_f))\n",
    "        unique_sequence_file = os.path.join(unique_sequence_dir, os.path.basename(_f))\n",
    "        \n",
    "        # collect clonotype/sequence information\n",
    "        with open(_f) as f:\n",
    "            for line in f:\n",
    "                data = line.strip().split(',')\n",
    "                if data[prod_field] == 'no':\n",
    "                    continue\n",
    "                v_gene = data[v_field]\n",
    "                j_gene = data[j_field]\n",
    "                cdr3_aa = data[cdr3aa_field]\n",
    "                vdj_nt = data[vdjnt_field]\n",
    "                clonotype_output_data.append(' '.join([v_gene, j_gene, cdr3_aa]))\n",
    "                sequence_output_data.append(' '.join([v_gene, j_gene, vdj_nt]))\n",
    "        \n",
    "        # write raw clonotype info to file\n",
    "        raw_clontype_string = '\\n'.join(clonotype_output_data)\n",
    "        with open(raw_clonotype_file, 'w') as rf:\n",
    "            rf.write(raw_clontype_string)\n",
    "        raw_clonotype_count = len(clonotype_output_data)\n",
    "        print('raw clonotypes:', raw_clonotype_count)\n",
    "        # collapse duplicate clonotypes (without counts)\n",
    "        uniq_cmd = 'sort -u -o {} -'.format(unique_clonotype_file)\n",
    "        p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True)\n",
    "        stdout, stderr = p.communicate(input=raw_clonotype_string)\n",
    "        # count the number of unique clonotypes\n",
    "        wc_cmd = 'wc -l {}'.format(unique_clonotype_file)\n",
    "        q = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "        _count, _ = q.communicate()\n",
    "        unique_clonotype_count = int(_count.split()[0])\n",
    "        print('unique clonotypes:', unique_clonotype_count)\n",
    "        if log_file is not None:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write('CLONOTYPES: {} {}\\n'.format(raw_clonotype_count, unique_clonotype_count))\n",
    "                \n",
    "        # write raw sequence info to file\n",
    "        raw_sequence_string = '\\n'.join(sequence_output_data)\n",
    "        with open(raw_sequence_file, 'w') as rf:\n",
    "            rf.write(raw_sequence_string)\n",
    "        raw_sequence_count = len(sequence_output_data)\n",
    "        print('raw sequences:', raw_sequence_count)\n",
    "        # collapse duplicate sequences (without counts)\n",
    "        uniq_cmd = 'sort -u -o {} -'.format(unique_sequence_file)\n",
    "        p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True)\n",
    "        stdout, stderr = p.communicate(input=raw_sequence_string)\n",
    "        # count the number of unique sequences\n",
    "        wc_cmd = 'wc -l {}'.format(unique_sequence_file)\n",
    "        q = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "        _count, _ = q.communicate()\n",
    "        unique_sequence_count = int(_count.split()[0])\n",
    "        print('unique sequences:', unique_sequence_count)\n",
    "        if log_file is not None:\n",
    "            with open(log_file, 'a') as f:\n",
    "                f.write('SEQUENCES: {} {}\\n'.format(raw_sequence_count, unique_sequence_count))\n",
    "        \n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the logfile\n",
    "with open(logfile, 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "# iteratively process each subject\n",
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    with open(logfile, 'a') as f:\n",
    "        f.write('#' + subject + '\\n')\n",
    "    files = list_files('./data/techrep-merged_minimal_no-header/{}'.format(subject))\n",
    "    raw_clonotype_dir = './data/techrep-merged_vj-aa/{}'.format(subject)\n",
    "    unique_clonptype_dir = './data/dedup_techrep-merged_vj-aa/{}'.format(subject)\n",
    "    raw_sequence_dir = './data/techrep-merged_vdj-nt/{}'.format(subject)\n",
    "    unique_sequence_dir = './data/dedup_techrep-merged_vdj-nt/{}'.format(subject)\n",
    "    dedup_bioreps(files, raw_clonotype_dir, unique_clonptype_dir,\n",
    "                  raw_sequence_dir, unique_sequence_dir, log_file=logfile)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (subject pools)\n",
    "\n",
    "In the previous blocks of code, we created a unique clonotype file for each biological replicate for each subject. Here, we'd like to create a single file for each subject containing only unique clonotypes (regardless of which biological replicate they came from)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dedup_clonotype_subject_pool_dir = './data/dedup_subject_clonotype_pools/'\n",
    "dedup_sequence_subject_pool_dir = './data/dedup_subject_sequence_pools/'\n",
    "make_dir(dedup_clonotype_subject_pool_dir)\n",
    "make_dir(dedup_sequence_subject_pool_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to create a unique clonotype file for each subject that also contains the number of times we saw each clonotype (using the deduplicated biological replicates, so the clonotype count essentially tallies the number of biological replicates in which we observed each clonotype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    \n",
    "    # clonotypes\n",
    "    input_clonotype_files = list_files(os.path.join(dedup_clonotype_dir, subject))\n",
    "    ofile = os.path.join(dedup_clonotype_subject_pool_dir, '{}_dedup_pool_vj-aa_with-counts.txt'.format(subject))\n",
    "    uniq_cmd = 'cat {} | sort | uniq -c > {}'.format(' '.join(input_clonotype_files), ofile)\n",
    "    c = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = c.communicate()\n",
    "    \n",
    "    # sequences\n",
    "    input_sequence_files = list_files(os.path.join(dedup_sequence_dir, subject))\n",
    "    ofile = os.path.join(dedup_sequence_subject_pool_dir, '{}_dedup_pool_vdj-nt_with-counts.txt'.format(subject))\n",
    "    uniq_cmd = 'cat {} | sort | uniq -c > {}'.format(' '.join(input_sequence_files), ofile)\n",
    "    s = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = s.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same process, but without counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    print(subject)\n",
    "        \n",
    "    # clonotypes\n",
    "    input_clonotype_files = list_files(os.path.join(dedup_clonotype_dir, subject))\n",
    "    ofile = os.path.join(dedup_clonotype_subject_pool_dir, '{}_dedup_pool_vj-aa.txt'.format(subject))\n",
    "    uniq_cmd = 'cat {} | sort | uniq > {}'.format(' '.join(input_clonotype_files), ofile)\n",
    "    c = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = c.communicate()\n",
    "    \n",
    "    # sequences\n",
    "    input_sequence_files = list_files(os.path.join(dedup_sequence_dir, subject))\n",
    "    ofile = os.path.join(dedup_sequence_subject_pool_dir, '{}_dedup_pool_vdj-nt.txt'.format(subject))\n",
    "    uniq_cmd = 'cat {} | sort | uniq > {}'.format(' '.join(input_sequence_files), ofile)\n",
    "    s = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = s.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplication (cross-subject pools)\n",
    "\n",
    "Finally, we'd like to create unique clonotype files (with counts) for every groupwise combination of our 10 subjects. Each group can contain two or more subjects, meaning the total number of possible groupwise combinations is quite large. We'll use the `multiprocessing` package to parallelize the process which should speed things up substantially, although even with parallelization, this will take some time.\n",
    "\n",
    "***NOTE:*** *The output from the following code blocks will be quite large (deduplicated clonotype files are >2TB in total, deduplicated sequence files are >20TB in total). Make sure you have sufficient storage and that the output paths below (`dedup_cross_subject_clonotype_pool_dir` and `dedup_cross_subject_sequence_pool_dir` are correct before starting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "dedup_cross_subject_clonotype_pool_dir = './data/dedup_cross-subject_clonotype_pools/'\n",
    "dedup_cross_subject_sequence_pool_dir = './data/dedup_cross-subject_sequence_pools/'\n",
    "make_dir(dedup_cross_subject_clonotype_pool_dir)\n",
    "make_dir(dedup_cross_subject_sequence_pool_dir)\n",
    "\n",
    "# deduplicated subject pool files\n",
    "dedup_clonotype_subject_files = [f for f in list_files(dedup_clonotype_subject_pool_dir) if '_dedup_pool_vj-aa.txt' in f]\n",
    "dedup_sequence_subject_files = [f for f in list_files(dedup_sequence_subject_pool_dir) if '_dedup_pool_vdj-nt.txt' in f]\n",
    "\n",
    "# every possible groupwise combination of subjects (2 or more subjects per group)\n",
    "subject_combinations_by_size = {}\n",
    "for size in range(2, 11):\n",
    "    subject_combinations_by_size[size] = [sorted(c) for c in itertools.combinations(subjects, size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_cross_subject_pool(subjects, files, output_dir):\n",
    "    files = sorted(list(set([f for f in dedup_subject_files if os.path.basename(f).split('_')[0] in subjects])))\n",
    "    output_file = os.path.join(output_dir, '{}_dedup_pool_vj-aa_with-counts.txt'.format('-'.join(subjects)))\n",
    "    uniq_cmd = 'cat {} | sort -T {} | uniq -c > {}'.format(' '.join(files), temp_dir, output_file)\n",
    "    p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clonotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "\n",
    "for size in sorted(subject_combinations_by_size.keys()):\n",
    "    subject_combinations = subject_combinations_by_size[size]\n",
    "    async_results = []\n",
    "    print('{}-subject pools:'.format(size))\n",
    "    progress_bar(0, len(subject_combinations))\n",
    "    for sub_comb in subject_combinations:\n",
    "        files = sorted(list(set([f for f in dedup_clonotype_subject_files if os.path.basename(f).split('_')[0] in sub_comb])))\n",
    "        async_results.append(p.apply_async(dedup_cross_subject_pool,\n",
    "                                           args=(sub_comb, files, dedup_cross_subject_clonotype_pool_dir)))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    print('\\n')\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequences\n",
    "\n",
    "Just one more warning that the following code block will produce a very large amount of data (>20TB) and will take many hours to run even on a fairly robust server (an `m4.16xlarge` AWS EC2 instance, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = mp.Pool(maxtasksperchild=1)\n",
    "\n",
    "for size in sorted(subject_combinations_by_size.keys()):\n",
    "    subject_combinations = subject_combinations_by_size[size]\n",
    "    async_results = []\n",
    "    print('{}-subject pools:'.format(size))\n",
    "    progress_bar(0, len(subject_combinations))\n",
    "    for sub_comb in subject_combinations:\n",
    "        files = sorted(list(set([f for f in dedup_sequence_subject_files if os.path.basename(f).split('_')[0] in sub_comb])))\n",
    "        async_results.append(p.apply_async(dedup_cross_subject_pool,\n",
    "                                           args=(sub_comb, files, dedup_cross_subject_sequence_pool_dir)))\n",
    "    monitor_mp_jobs(async_results)\n",
    "    print('\\n')\n",
    "\n",
    "p.close()\n",
    "p.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
