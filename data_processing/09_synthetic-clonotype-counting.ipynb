{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clonotype deduplication (synthetic repertoires)\n",
    "\n",
    "Starting with annotated sequence data (in AbStar's `minimal` output format), reduces sequences to clonotypes and collapses dupicate clonotypes.\n",
    "\n",
    "The [`abutils`](https://www.github.com/briney/abutils) Python package is required, and can be installed by running `pip install abutils`\n",
    "\n",
    "*NOTE: this notebook requires the use of the Unix command line tool `sort`. Thus, it requires a Unix-based operating system to run correctly (MacOS and most flavors of Linux should be fine). Running this notebook on Windows 10 may be possible using the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about) but we have not tested this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import itertools\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from abutils.utils.jobs import monitor_mp_jobs\n",
    "from abutils.utils.pipeline import list_files, make_dir\n",
    "from abutils.utils.progbar import progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subjects, directories and data fields\n",
    "\n",
    "The input data (annotated synthetic sequences in [abstar's](https://github.com/briney/abstar) `minimal` format) is too large to be stored in a Github repository. The two sythetic datasets can be downloaded from the following links:\n",
    "\n",
    "  * default IGoR recombination model: [**DOWNLOAD**](http://burtonlab.s3.amazonaws.com/GRP_github_data/synthetic_default-model_minimal.tar.gz)\n",
    "  * subject-specific IGoR recombination models: [**DOWNLOAD**](http://burtonlab.s3.amazonaws.com/GRP_github_data/synthetic_subject-specific-models_minimal.tar.gz)\n",
    "\n",
    "The datasets are fairly large (each dataset is approximately 1TB uncompressed), so make sure you have enough space before downloading. Decompressing the default IGoR recombination model archive from within the `data` directory (located in the same parent directory as this notebook) will allow the code in this notebook to run without modification. If you would prefer to store the input data somewhere else or would like to use the subject-specific IGoR model data instead, be sure to modify the `raw_input_dir` path below.\n",
    "\n",
    "The data fields defined below correspond to the prosition in abstar's `minimal` format. If for some reason you have a differently formatted annotation file, change the field positions to suit your annotation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects\n",
    "with open('./data/subjects.txt') as f:\n",
    "    subjects = sorted(f.read().split())\n",
    "\n",
    "# directories\n",
    "raw_input_dir = './data/synthetic_default-model_minimal/'\n",
    "raw_clonotype_dir = './data/synthetic_default-model_vj-aa/'\n",
    "unique_clonotype_dir = './data/dedup_synthetic_default-model_vj-aa/'\n",
    "counts_clonotype_dir = './data/dedup_synthetic_default-model_vj-aa_with-counts/'\n",
    "temp_dir = './data/temp'\n",
    "logfile = './data/dedup.log'\n",
    "\n",
    "# make directories\n",
    "make_dir(raw_clonotype_dir)\n",
    "make_dir(unique_clonotype_dir)\n",
    "make_dir(counts_clonotype_dir)\n",
    "make_dir(temp_dir)\n",
    "with open(logfile, 'w') as f:\n",
    "    f.write('')\n",
    "\n",
    "# data fields\n",
    "prod_field = 3\n",
    "v_field = 5\n",
    "j_field = 9\n",
    "cdr3aa_field = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deduplication\n",
    "\n",
    "For each synthetic sequence datafile, we'd like to create three new clonotype files:\n",
    "  1. a file containing the raw clonotypes, one for every productive sequence\n",
    "  2. a file containing just unique clonotypes, used to quantify cross-sample clonotype sharing\n",
    "  3. a file containing unique clonotypes with counts (the number of times each unique clonotype was observed), used to quantify repeat observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup_clonotypes(minimal_file):\n",
    "      \n",
    "    # process minimal file\n",
    "    print(os.path.basename(minimal_file))\n",
    "    clonotype_output_data = []\n",
    "    sequence_output_data = []\n",
    "    raw_clonotype_file = os.path.join(raw_clonotype_dir, os.path.basename(minimal_file))\n",
    "    unique_clonotype_file = os.path.join(unique_clonotype_dir, os.path.basename(minimal_file))\n",
    "    counts_clonotype_file = os.path.join(counts_clonotype_dir, os.path.basename(minimal_file))\n",
    "\n",
    "    # collect clonotype/sequence information\n",
    "    with open(minimal_file) as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split(',')\n",
    "            if data[prod_field] == 'no':\n",
    "                continue\n",
    "            v_gene = data[v_field]\n",
    "            j_gene = data[j_field]\n",
    "            cdr3_aa = data[cdr3aa_field]\n",
    "            clonotype_output_data.append(' '.join([v_gene, j_gene, cdr3_aa]))\n",
    "\n",
    "    # write raw clonotype info to file\n",
    "    raw_clontype_string = '\\n'.join(clonotype_output_data)\n",
    "    with open(raw_clonotype_file, 'w') as rf:\n",
    "        rf.write(raw_clontype_string)\n",
    "    raw_clonotype_count = len(clonotype_output_data)\n",
    "    print('raw clonotypes:', raw_clonotype_count)\n",
    "    \n",
    "    # collapse duplicate clonotypes (without counts)\n",
    "    uniq_cmd = 'sort -u -o {} -'.format(unique_clonotype_file)\n",
    "    p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate(input=raw_clonotype_string)\n",
    "    \n",
    "    # collapse duplicate_clonotypes (with counts)\n",
    "    uniq_cmd = 'sort -T {} | uniq -c > {}'.format(temp_dir,\n",
    "                                                  counts_clonotype_file)\n",
    "    p = sp.Popen(uniq_cmd, stdout=sp.PIPE, stderr=sp.PIPE, stdin=sp.PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate(input=raw_clonotype_string)\n",
    "    \n",
    "    # count the number of unique clonotypes\n",
    "    wc_cmd = 'wc -l {}'.format(unique_clonotype_file)\n",
    "    q = sp.Popen(wc_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    _count, _ = q.communicate()\n",
    "    unique_clonotype_count = int(_count.split()[0])\n",
    "    print('unique clonotypes:', unique_clonotype_count)\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write('CLONOTYPES: {} {}\\n'.format(raw_clonotype_count, unique_clonotype_count))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for minimal_file in list_files(raw_input_dir):\n",
    "    dedup_clonotypes(minimal_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
