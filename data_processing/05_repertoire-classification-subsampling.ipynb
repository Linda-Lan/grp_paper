{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repertoire classification subsampling\n",
    "\n",
    "When training a classifier to assign repertoires to the subject from which they were obtained, we need a set of subsampled sequences. The sequences have been condensed to just the V- and J-gene assignments and the CDR3 length (VJ-CDR3len). Subsample sizes range from 10 to 10,000 sequences per biological replicate.\n",
    "\n",
    "The [`abutils`](https://www.github.com/briney/abutils) Python package is required for this notebook, and can be installed by running `pip install abutils`.\n",
    "\n",
    "*NOTE: this notebook requires the use of the Unix command line tool `shuf`. Thus, it requires a Unix-based operating system to run correctly (MacOS and most flavors of Linux should be fine). Running this notebook on Windows 10 may be possible using the [Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about) but we have not tested this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from collections import Counter\n",
    "import os\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import tempfile\n",
    "\n",
    "from abutils.utils.pipeline import make_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subjects, subsample sizes, and directories\n",
    "\n",
    "The `input_dir` should contain deduplicated clonotype sequences. The datafiles are too large to be included in the Github repository, but may be downloaded [**here**](http://burtonlab.s3.amazonaws.com/GRP_github_data/techrep-merged_vj-cdr3len_no-header.tar.gz). If downloading the data (which will be downloaded as a compressed archive), decompress the archive in the `data` directory (in the same parent directory as this notebook) and you should be ready to go. If you want to store the downloaded data in some other location, adjust the `input_dir` path below as needed.\n",
    "\n",
    "By default, subsample sizes increase by 10 from 10 to 100, by 100 from 100 to 1,000, and by 1,000 from 1,000 to 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/subjects.txt') as f:\n",
    "    subjects = sorted(f.read().split())\n",
    "\n",
    "subsample_sizes = list(range(10, 100, 10)) + list(range(100, 1000, 100)) + list(range(1000, 11000, 1000))\n",
    "\n",
    "input_dir = './data/techrep-merged_vj-cdr3len_no-header/'\n",
    "subsample_dir = './data/repertoire_classification/user-created_subsamples_vj-cdr3len'\n",
    "make_dir(subsample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(infile, outfile, n_seqs, iterations):\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write('')\n",
    "    shuf_cmd = 'shuf -n {} {}'.format(n_seqs, infile)\n",
    "    p = sp.Popen(shuf_cmd, stdout=sp.PIPE, stderr=sp.PIPE, shell=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    with open(outfile, 'a') as f:\n",
    "        for iteration in range(iterations):\n",
    "            seqs = ['_'.join(s.strip().split()) for s in stdout.strip().split('\\n') if s.strip()]\n",
    "            counts = Counter(seqs)\n",
    "            count_strings = []\n",
    "            for k, v in counts.items():\n",
    "                count_strings.append('{}:{}'.format(k, v))\n",
    "            f.write(','.join(count_strings) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subject in subjects:\n",
    "    print(subject)\n",
    "    files = list_files(os.path.join(input_dir, subject))\n",
    "    for file_ in files:\n",
    "        for subsample_size in subsample_sizes:\n",
    "            num = os.path.basename(file_).split('_')[0]\n",
    "            ofile = os.path.join(subsample_dir, '{}_{}-{}'.format(subject, subsample_size, num))\n",
    "            subsample(file_, ofile, subsample_size, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
